
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>AudioRecorder Walkthrough: Web Audio, WebRTC, and Web Workers - Basic Object</title>
  <meta name="author" content="D. Flaherty">

  
  <meta name="description" content="As a musician, engineer, and web developer, it has been very exciting to watch the Web Audio and WebRTC APIs mature. A robust platform for audio on &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://flats.github.io/blog/2016/01/25/audiorecorder-walkthrough-web-audio-and-webrtc">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Basic Object" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='//fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='//fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>

  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner">
	<div class="header-title"><a href="/">Basic Object</a></div>


	<br><div class="header-subtitle">Deep thoughts on web programming</div>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:flats.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
  
    
      <h1 class="entry-title">AudioRecorder Walkthrough: Web Audio, WebRTC, and Web Workers</h1>
    
  
    
      <p class="meta">
        








  


<time datetime="2016-01-25T13:09:44-05:00" pubdate data-updated="true"></time>
        
      </p>
    
  </header>


<div class="entry-content"><p>As a musician, engineer, and web developer, it has been very exciting to watch the Web Audio and WebRTC APIs mature. A robust platform for audio on the web has the potential to usher in a new era of collaborative music creation and social audio sharing. While native desktop DAW applications such as Pro Tools, Logic, Ableton Live, FL Studio, GarageBand, Digital Performer, and Cubase have grown more powerful and flexible over the last several years, facilities for collaboration and social sharing haven’t really kept pace.<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> Perhaps the DAW of the future will be a web application, something like the audio equivalent of Google Docs.</p>

<p>I can dream. Or I can take things apart, see how they work, and then build new things.</p>

<h2 id="recorderjs">recorder.js</h2>

<p>In this post, I’ll walk through a popular demo of recording with the Web Audio API. In the process, I’ll touch on the basics of the Web Audio API, Web Workers, and WebRTC, a trio of fledgling web technologies that combine to allow us to record and process audio in the brower.</p>

<p>The <a href="https://webaudiodemos.appspot.com/AudioRecorder/index.html">AudioRecorder demo</a> is simple recording interface comprising three parts: an animated spectrograph of the client’s current audio input, a record button that starts and stops recording audio (and displaying the recorded audio’s waveform) from that input, and a save button that allows the user to download the recorded audio as a WAV file. I’m just going to focus on the latter two here, those responsible for recording audio, displaying the audio’s waveform while recording, and finally saving the recording. Most of this functionality comes from <a href="https://github.com/mattdiamond/Recorderjs">Matt Diamond’s fantastic recorder.js</a> plugin.</p>

<h2 id="part-1-mainjs">Part 1: main.js</h2>

<p>You can <a href="https://webaudiodemos.appspot.com/AudioRecorder/js/main.js">follow along with main.js here</a>.</p>

<p>Like most JavaScript that takes advantage of bleeding-edge browser features, this application’s Javascript begins with a whole bunch of feature detection to make sure that the browser is capable of animating the audio display and obtaining an audio stream using WebRTC. At this time, the WebRTC <a href="http://caniuse.com/#feat=rtcpeerconnection">is partially implemented in some browsers and not supported at all in several browsers</a>. It provides access to a video and/or audio stream from the user using <code>navigator.getUserMedia()</code>, which takes a parameter of <code>constraints</code>. The constraints are either <code>optional</code> or <code>mandatory</code>. In this case, the recorder is using the <code>mandatory</code> constraints to disable a handful of default audio behaviors in Chrome.</p>

<p>Now we get into the Web Audio API, which allows us to work with audio in the browser. It <a href="http://caniuse.com/#feat=audio-api">is quite well-supported in modern browsers</a>. Much as the HTML <code>&lt;canvas&gt;</code> element allows us to get a 2D or 3D drawing context in JavaScript, the <code>AudioContext()</code> constructor gives us a context in which we can connect sound sources and destinations (which the Web Audio API collectively calls <code>AudioNode</code>s). Everything that the Web Audio API does happens within AudioContext objects, which are directed graphs of AudioNodes that are responsible for receiving, producing, transforming, and emitting audio signals.<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> In this way, it works similarly to my beloved Minimoog compact modular synthesizer from the 1970s, with its oscillators, mixer, and filters, or to a recording studio with its many microphones, processors (EQ, compression, effects, and so on), mixing console, and recording devices (tape machines and computers).</p>

<div class="bogus-wrapper"><notextile><figure>
  <img src="/images/minimoog800x600.jpg" alt="My beloved Model D Minimoog" title="Minimoog Model D Control Panel" />  
  <figcaption style="width: 100%; margin-bottom: 2.25em; margin-top: 1em; font-size: .8em; text-align: center;">The control panel of my beloved Model D Minimoog.</figcaption>
</figure></notextile></div>

<p>In this case, the callback passed to <code>getUserMedia()</code>, <code>gotStream()</code>, is called when the stream is successfully obtained and goes about creating a bunch of AudioNodes on the context and connecting them. There’s an <code>inputPoint</code> node for controlling gain, a <code>realAudioInput</code> for actually getting the audio, an <code>analyserNode</code> for performing a Fourier transform on the audio (this is for displaying the spectrograph or “analyser”), and finally a <code>zeroGain</code> node that is the endpoint of the audio in the context. The recorder just records audio and doesn’t actually output any audio directly, so the <code>zeroGain</code> just receives the audio at the end of the chain and silences it. Right before that connection is made, a new <code>Recorder</code> object is created with the stream from <code>getUserMedia()</code> as its source.</p>

<div class="bogus-wrapper"><notextile><figure>
  <img src="/images/audio-graph.png" alt="Graphical representation of audio graph" title="AudioRecorder Audio Graph" />  
  <figcaption style="width: 100%; margin-bottom: 2.25em; margin-top: 1em; font-size: .8em; text-align: center;">A graphical representation of the audio graph created in <em>gotStream()</em>.</figcaption>
</figure></notextile></div>

<p>Okay, so that covers the initialization of the application. The rest of the application, save the analyzer that I won’t cover in detail here, is event driven. The <code>toggleRecord()</code> function is triggered by the click event on the record button, which in turn either stops or starts the recording.</p>

<p>We’ll start with the recorder activation. <code>clear()</code> is called on the instance of <code>Recorder</code> created in <code>gotStream()</code>, followed by <code>record()</code>. The methods are implemented in Matt Diamond’s recorder.js.</p>

<h2 id="part-2-recorderjs">Part 2: recorder.js</h2>

<p>The Web Audio API provides many pre-built AudioNode objects that cover most use cases that are generally written in a lower-level language than JavaScript (usually C or C++). However, it’s also possible to create your own processing object directly in JavaScript using the <code>createScriptProcessor()</code> method of <code>AudioContext</code>, which is how Mr. Diamond deals with recording audio in recorder.js.</p>

<p>In addition, Mr. Diamond’s recorder.js makes use of the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers">Web Worker API</a>, another relatively new browser API that gives developers the ability to run background processes. The API, which <a href="http://caniuse.com/#feat=webworkers">is quite well-supported in modern browsers</a>, exposes a constructor function, <code>Worker()</code>, that takes an argument of the JavaScript file that will run on its own thread in the background. The worker in recorder.js is assigned to <a href="https://webaudiodemos.appspot.com/AudioRecorder/js/recorderjs/recorderWorker.js">the recorderWorker.js file</a>, which we’ll get to in a moment.</p>

<p>The <code>Recorder()</code> constructor begins by creating a script processor and setting a buffer length for recording audio. Communication between a worker and the script that spawned it is achieved through message passing, so this function then passes the ‘init’ message to the worker along with a config object that specifies the sample rate.</p>

<p>The worker script contains all of the guts of the recorder. The entire process of passing audio from the left and right channel input buffers to the recording buffers and creating a WAV file from those buffers is encapsulated in this worker. The <code>switch</code> statement on the ‘onmessage’ event at the top of the function, which receives the ‘command’ and ‘buffer’ parameters sent from recorder.js, controls the workers behavior - the ‘record’, ‘exportWAV’, ‘getBuffers’, and ‘clear’ messages kick off those actions in the worker.</p>

<h2 id="intermission">Intermission</h2>

<p>I’ll be updating this post with more details on this worker and the recorder.js flow at a later date. In the meantime, make some noise at your laptop.</p>

<h3 id="more-resources">More Resources</h3>

<p>Web Audio API at MDN: <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API">https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API</a></p>

<p>Tutorial at HTML5 Rocks: <a href="http://www.html5rocks.com/en/tutorials/webaudio/intro/">http://www.html5rocks.com/en/tutorials/webaudio/intro/</a></p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Soundcloud and Bandcamp are fantastic platforms, but because neither of them provide any creation tools (unlike platforms like Instagram and Vine), they’re largely out of reach for non-professional users. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>There’s more about the audio graph in Boris Smus’s <a href="http://chimera.labs.oreilly.com/books/1234000001552">excellent book on the Web Audio API</a>. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">D. Flaherty</span></span>

      








  


<time datetime="2016-01-25T13:09:44-05:00" pubdate data-updated="true"></time>
      


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://flats.github.io/blog/2016/01/25/audiorecorder-walkthrough-web-audio-and-webrtc/" data-via="" data-counturl="http://flats.github.io/blog/2016/01/25/audiorecorder-walkthrough-web-audio-and-webrtc/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/01/03/frozen-strings/" title="Previous Post: Frozen Strings, Symbols, and Garbage Collection in Ruby">&laquo; Frozen Strings, Symbols, and Garbage Collection in Ruby</a>
      
      
        <a class="basic-alignment right" href="/blog/2016/02/07/sets-and-set-operators-in-ruby/" title="Next Post: Sets and Set Operators in Ruby">Sets and Set Operators in Ruby &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/02/24/custom-ember-leaflet-2-dot-0-components/">Custom Ember Leaflet 2.0 Components</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/02/07/sets-and-set-operators-in-ruby/">Sets and Set Operators in Ruby</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/25/audiorecorder-walkthrough-web-audio-and-webrtc/">AudioRecorder Walkthrough: Web Audio, WebRTC, and Web Workers</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/03/frozen-strings/">Frozen Strings, Symbols, and Garbage Collection in Ruby</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/12/10/closures-yield-and-the-enumerable-module/">Closures, Yield, and the Enumerable Module</a>
      </li>
    
  </ul>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 -  D. Flaherty <br/>
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> + <a href="https://github.com/ioveracker/mnml">mnml</a>.
	  
  </span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
